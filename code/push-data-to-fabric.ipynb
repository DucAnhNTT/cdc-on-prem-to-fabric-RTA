{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent_kafka in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent_kafka pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import base64\n",
    "from decimal import Decimal\n",
    "\n",
    "# Kafka On-prem Configuration\n",
    "bootstrap_servers_local = 'localhost:9092'\n",
    "topic_local = 'cdc.classicmodels.orderdetails'\n",
    "group_id_local = 'my_consumer_group'\n",
    "\n",
    "# Kafka Cloud Configuration\n",
    "connection_string = \"your-connection-string\"\n",
    "namespace = connection_string.split(\"sb://\")[1].split(\".\")[0]\n",
    "bootstrap_servers_cloud = f\"{namespace}.servicebus.windows.net:9093\"\n",
    "sasl_mechanism = 'PLAIN'\n",
    "security_protocol = 'SASL_SSL'\n",
    "sasl_plain_username = '$ConnectionString'\n",
    "sasl_plain_password = connection_string\n",
    "event_hub_name = \"your-EH-name\"\n",
    "\n",
    "# Kafka consumer configuration (On-prem)\n",
    "conf_local = {\n",
    "    'bootstrap.servers': bootstrap_servers_local,\n",
    "    'group.id': group_id_local,\n",
    "    'auto.offset.reset': 'earliest'  # read from beginning of topic\n",
    "}\n",
    "\n",
    "# Kafka producer configuration (Cloud)\n",
    "producer_cloud = KafkaProducer(\n",
    "    bootstrap_servers=bootstrap_servers_cloud,\n",
    "    security_protocol=security_protocol,\n",
    "    sasl_mechanism=sasl_mechanism,\n",
    "    sasl_plain_username=sasl_plain_username,\n",
    "    sasl_plain_password=sasl_plain_password,\n",
    "    key_serializer=lambda v: str(v).encode('utf-8'),\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Create Kafka Consumer instance (On-prem)\n",
    "consumer_local = Consumer(conf_local)\n",
    "consumer_local.subscribe([topic_local])\n",
    "\n",
    "# Initialize an empty list to store extracted data\n",
    "data_list = []\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Poll for messages\n",
    "        msg = consumer_local.poll(1.0)  # 1 second timeout\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition\n",
    "                print(f\"Reached end of partition {msg.partition()}\")\n",
    "                continue\n",
    "            else:\n",
    "                # Error\n",
    "                print(f\"Error: {msg.error()}\")\n",
    "                break\n",
    "        else:\n",
    "            # Decode message value\n",
    "            kafka_message = msg.value().decode('utf-8')\n",
    "            print(f\"Received message: {kafka_message}\")\n",
    "\n",
    "            # Parse JSON message\n",
    "            message = json.loads(kafka_message)\n",
    "            \n",
    "            # Extract necessary fields from payload\n",
    "            payload = message['payload']\n",
    "            after_data = payload['after']\n",
    "            orderNumber = after_data['orderNumber']\n",
    "            productCode = after_data['productCode']\n",
    "            quantityOrdered = after_data['quantityOrdered']\n",
    "            priceEach_base64 = after_data['priceEach']\n",
    "            orderLineNumber = after_data['orderLineNumber']\n",
    "\n",
    "            # Decode priceEach from base64 and convert to decimal\n",
    "            priceEach_bytes = base64.b64decode(priceEach_base64)\n",
    "            priceEach = Decimal(int.from_bytes(priceEach_bytes, byteorder='big')) / Decimal(100)  # Adjust scale\n",
    "\n",
    "            payload['after']['priceEach'] = float(priceEach)\n",
    "\n",
    "            # Append extracted data to list\n",
    "            data_list.append({\n",
    "                'OrderNumber': orderNumber,\n",
    "                'ProductCode': productCode,\n",
    "                'QuantityOrdered': quantityOrdered,\n",
    "                'PriceEach': float(priceEach),\n",
    "                'OrderLineNumber': orderLineNumber\n",
    "            })\n",
    "\n",
    "            # Send only the payload to cloud Kafka (Azure Event Hub)\n",
    "            producer_cloud.send(event_hub_name, key=str(orderNumber), value=after_data)\n",
    "            producer_cloud.flush()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "finally:\n",
    "    # Close consumer\n",
    "    consumer_local.close()\n",
    "    # Close producer\n",
    "    producer_cloud.close()\n",
    "\n",
    "# Convert list of dictionaries to DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"DataFrame from Kafka messages:\")\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
